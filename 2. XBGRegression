# Data structuring
------------------
# dictionary for collecting the test data forecast results
test_forecast_dict_xgb = dict()

# dictionary for collecting the evaluation metrics of the test forecast
em_dict_xgb = dict()

# Model building & forecasting
------------------------------
# running a for loop with a list of unique items in the dataset as the iterable
for i in list(md['item'].unique()):

  # filtering the data item-wise
  data = md[md['item']==i].drop(['item'], axis=1)
  #print(data)
  
  # Splitting the dataset into dependent and independent variable
  x = data.drop(['sales'], axis=1)
  y = data['sales']
   
  # Splitting data into training and testing sets
  # Here, we are limiting the test data upto 02/10/2017 because item no 50 is missing 90 days of data as a result of
  # rolling(90), shift(-90) and dropna() Hence, extending the prediction upto 31/12/2017 is resulting in a Value Error.
  
  x_train = x.loc[:'2016-12-31']
  x_test = x.loc['2017-01-01':'2017-10-02'] 
  y_train = y.loc[:'2016-12-31']
  y_test = y.loc['2017-01-01':'2017-10-02']

  # Model Building
  # Xtreme Gradient Boosting

  # Initializing the model
  xgbr = XGB(verbosity=0)
  
  # Fitting the data to the model
  xgbr.fit(x_train, y_train)

  # computing training score
  #train_score = xgbr.score(x_train, y_train)
  #cvs_train = cross_val_score(xgbr, x_train, y_train,cv=10)
 
  # predicting the test data
  y_pred = xgbr.predict(x_test)
  
  # creating key and value for test_forecast dictionary
  k = 'Item_'+str(i)
  v = y_pred

  # dictionary for test data
  test_forecast_dict_xgb.update({k:v})

  # Statistics
  Mean = data['sales'].mean()
  Std_dev = data['sales'].std()

  # Evaluation Metrics
  r2 = r2_score(y_test, y_pred)
  mse = mean_squared_error(y_test, y_pred)
  rmse = np.sqrt(mse)
  mae = mean_absolute_error(y_test, y_pred)
  mape = mean_absolute_percentage_error(y_test, y_pred)

  # creating a key and value for test_forecast evaluation metrics dictionary
  key = 'Item_'+str(i)
  value = {'r2':r2, 'MSE':mse, 'RMSE':rmse, 'MAE':mae, 'MAPE':mape}
  em_dict_xgb.update({key:value})

  # Data Visualization
  plt.figure(figsize=(6,3))
  plt.title('Item_'+str(i))
  plt.plot(y_pred, label='Test')    # test data 
  #plt.plot(data.sales, label='Actual')  # actual sales
  plt.xlabel('Date', size=10)
  plt.ylabel('Sales #', size=10)
  plt.xticks(rotation=45)
  plt.yticks(rotation=45)
  plt.legend(loc='best')
  plt.show()
  
# Dataframe for test data forecast
----------------------------------
test_forecast_dict_xgb_df = pd.DataFrame(test_forecast_dict_xgb, index=[pd.date_range(start='2017-01-01', end='2017-10-02')])

# function for rounding off
def rounding_xgb(x):
  a = round(x,2)
  return a

for i in test_forecast_dict_xgb_df.columns:
  test_forecast_dict_xgb_df[i] = test_forecast_dict_xgb_df[i].apply(lambda x:rounding_xgb(x))

test_forecast_dict_xgb_df = test_forecast_dict_xgb_df.reset_index()
test_forecast_dict_xgb_df

test_forecast_dict_xgb_df = test_forecast_dict_xgb_df.set_index('level_0', drop=True)
test_forecast_dict_xgb_df.index.name=None

# Dictionary for performance evaluation metrics
em_dict_xgb_df = pd.DataFrame(em_dict_xgb)

# rounding the results
------------------------------------------------
def rounding_em(x):
  em = round(x,2)
  return em

for i in em_dict_xgb_df.columns:
  em_dict_xgb_df[i] = em_dict_xgb_df[i].apply(lambda x:rounding_em(x))
  
# plotting forecast and actual sales
-------------------------------------
for k, v in test_forecast_dict_xgb.items():
  forecast = pd.DataFrame(v, index=pd.date_range(start='2017-01-01', end='2017-10-02'))
  plt.figure(figsize=(6,3))
  plt.title(k)
  plt.plot(forecast, label='Test')
  plt.plot(data.sales, label='Actual')
  plt.xlabel('Date', size=12)
  plt.ylabel('Sales #', size=12)
  plt.xticks(rotation=45, size=10)
  plt.yticks(rotation=45, size=10)
  plt.legend(loc='best')
  plt.show()

# creating a dupllicate dataframe for computing the losses
-----------------------------------------------------------
dfxgb = master_data.copy()
dfxgb = dfxgb.groupby(['item','date']).agg({'sales':'sum'}).reset_index().set_index('date', drop=True)
dfxgb = dfxgb.groupby(['item']).sales.rolling(90).sum().shift(-90).dropna().reset_index()
dfxgb = pd.pivot_table(data=dfxgb, columns ='item', values='sales', index='date', aggfunc='sum')
dfxgb.index.name=None
dfxgb.columns.name=None
dfxgb_column_names = ['Item_'+str(i) for i in dfxgb.columns]
dfxgb.columns = dfxgb_column_names
dfxgb

# Oppurtunity loss & Retention loss
------------------------------------
# dfxgb - Actual sales (demand)
# test_forecast_dict_xgb_df - Forecast (supply)
# (Forecast - Actual) = (Supply - Demand) > 0 => Rs 10 else Rs 3

results_xgb = (test_forecast_dict_xgb_df - dfxgb).dropna()

# Actual sales = Demand
# Forecast sales = Supply
# When demand > supply = Oppurtunity loss - multiply by Rs 3
# When demand < supply = Retention loss - multiplt by Rs 10

def orloss(x):
  if x > 0:
    x = x*10
    return x
  else:
    x = x*3
    return x

for i in results_xgb.columns:
  results_xgb[i] = results_xgb[i].apply(lambda x: orloss(x))

results_xgb
